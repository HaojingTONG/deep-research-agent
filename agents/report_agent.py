"""
Phase 6: Report Agent

Generates structured decision-grade reports with citations.
Creates audience-tailored reports with executive summaries and recommendations.
"""

import re
from typing import Dict, Any, List
from datetime import datetime
from utils import extract_topic_keywords


class ReportAgent:
    """
    Phase 6: Report Agent - Generate structured decision-grade reports with citations.

    Creates comprehensive markdown reports from compressed analysis and evidence,
    tailored for different audiences with executive summaries, insights, and recommendations.
    """

    def __init__(self):
        """Initialize the ReportAgent."""
        pass

    def generate_report(self, compressed_json: Dict[str, Any], evidence_list: List[Dict[str, Any]],
                       user_query: str, target_audience: str = "consumer") -> str:
        """
        Generate structured Markdown report from compressed analysis and evidence.

        Creates comprehensive reports with executive summaries, key insights, timeline
        analysis, recommendations, and complete evidence references.

        Args:
            compressed_json: Output from CompressConflictAgent with findings and conflicts
            evidence_list: Full list of evidence blocks for citations
            user_query: Original user query for topic extraction
            target_audience: Target audience (consumer, executive, researcher)

        Returns:
            Structured Markdown report with citations and audience-appropriate content
        """
        # Extract information from compressed analysis
        key_findings = compressed_json.get("key_findings", [])
        conflicts = compressed_json.get("conflicts", [])
        gaps = compressed_json.get("gaps", [])
        coverage_stats = compressed_json.get("coverage_stats", {})
        clusters = compressed_json.get("clusters", [])

        # Generate report sections
        executive_summary = self._generate_executive_summary(key_findings, conflicts, target_audience, user_query)
        key_insights = self._generate_key_insights(key_findings, clusters, evidence_list, target_audience, user_query)
        timeline_section = self._generate_timeline_section(evidence_list, target_audience)
        recommendations = self._generate_recommendations(key_findings, conflicts, gaps, target_audience, user_query)
        limitations = self._generate_limitations(conflicts, gaps, coverage_stats)

        # Build complete report
        report = f"""# Research Report: {self._extract_topic_from_query(user_query)}

**Target Audience:** {target_audience.title()}
**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M")}
**Evidence Sources:** {coverage_stats.get('unique_sources', 0)} unique sources across {coverage_stats.get('domains', 0)} domains

---

## Executive Summary

{executive_summary}

---

## Key Insights

{key_insights}

---

{timeline_section}

---

## Recommendations

{recommendations}

---

## Limitations & Open Questions

{limitations}

---

## Evidence References

{self._generate_evidence_references(evidence_list)}

---

*Report generated by Deep Research Agent*
"""

        return report

    def _extract_topic_keywords(self, user_query: str) -> List[str]:
        """Extract normalized topic keywords from user query."""
        return extract_topic_keywords(user_query)

    def _get_topic_context(self, user_query: str) -> Dict[str, str]:
        """Extract topic context for adaptive language generation."""
        keywords = self._extract_topic_keywords(user_query)
        query_lower = user_query.lower()

        # Determine topic domain and appropriate terminology
        if any(term in query_lower for term in ['artificial intelligence', 'ai', 'machine learning']) and 'education' in query_lower:
            return {
                'domain': 'ai_education',
                'main_topic': 'AI in education',
                'keywords': keywords,
                'opportunities_term': 'opportunities',
                'risks_term': 'challenges',
                'guidance_term': 'implementation guidance',
                'subject_area': 'educational technology'
            }
        elif any(term in query_lower for term in ['climate', 'environment', 'sustainability']):
            return {
                'domain': 'climate',
                'main_topic': 'climate and environment',
                'keywords': keywords,
                'opportunities_term': 'solutions',
                'risks_term': 'risks',
                'guidance_term': 'policy recommendations',
                'subject_area': 'environmental policy'
            }
        elif any(term in query_lower for term in ['social media', 'mental health', 'teenagers', 'adolescents', 'psychology']):
            return {
                'domain': 'psychology',
                'main_topic': 'social media and mental health',
                'keywords': keywords,
                'opportunities_term': 'benefits',
                'risks_term': 'risks',
                'guidance_term': 'wellness recommendations',
                'subject_area': 'mental health research'
            }
        elif any(term in query_lower for term in ['food', 'nutrition', 'diet', 'dietary']) and not any(term in query_lower for term in ['social media', 'mental health']):
            return {
                'domain': 'health',
                'main_topic': 'health and nutrition',
                'keywords': keywords,
                'opportunities_term': 'benefits',
                'risks_term': 'risks',
                'guidance_term': 'health recommendations',
                'subject_area': 'health policy'
            }
        else:
            # Generic context
            topic_phrase = ' '.join(keywords[:2]) if keywords else 'the research topic'
            return {
                'domain': 'general',
                'main_topic': topic_phrase,
                'keywords': keywords,
                'opportunities_term': 'opportunities',
                'risks_term': 'challenges',
                'guidance_term': 'recommendations',
                'subject_area': 'research area'
            }

    def _extract_topic_from_query(self, user_query: str) -> str:
        """Extract main topic from user query for report title."""
        query_lower = user_query.lower()

        if "ultra-processed foods" in query_lower or "upf" in query_lower:
            return "Ultra-Processed Foods Evidence Analysis"
        elif "ai safety" in query_lower:
            return "AI Safety in Autonomous Vehicles"
        elif "climate change" in query_lower:
            return "Climate Change Research Analysis"
        else:
            # Generic title
            return "Research Evidence Analysis"

    def _generate_executive_summary(self, key_findings: List[str], conflicts: List[Dict],
                                  target_audience: str, user_query: str) -> str:
        """Generate 5-bullet executive summary tailored to audience and topic."""
        summary_bullets = []
        topic_context = self._get_topic_context(user_query)
        main_topic = topic_context['main_topic']
        opportunities_term = topic_context['opportunities_term']
        risks_term = topic_context['risks_term']
        guidance_term = topic_context['guidance_term']
        subject_area = topic_context['subject_area']

        # Bullet 1: Main research scope and coverage - topic-specific
        summary_bullets.append(f"• **Research Scope**: Comprehensive analysis of recent evidence on {main_topic} with systematic evaluation of quality and relevance")

        # Bullet 2: Key positive findings - extract from actual data
        positive_findings = [f for f in key_findings if any(word in f.lower() for word in ["benefit", "positive", "advantage", "opportunity", "improvement", "effective"])]
        if positive_findings:
            # Clean up the finding text to be topic-neutral
            finding_text = positive_findings[0].split('[')[0].strip()
            # Replace food-specific terms with topic-appropriate ones
            finding_text = self._adapt_finding_to_topic(finding_text, topic_context)
            summary_bullets.append(f"• **{opportunities_term.title()}**: {finding_text}")
        else:
            summary_bullets.append(f"• **Limited {opportunities_term.title()}**: Current evidence shows limited documented {opportunities_term} in {subject_area}")

        # Bullet 3: Key concerns or risks - extract from actual data
        risk_findings = [f for f in key_findings if any(word in f.lower() for word in ["risk", "harmful", "disease", "negative", "challenge", "concern", "limitation"])]
        if risk_findings:
            finding_text = risk_findings[0].split('[')[0].strip()
            finding_text = self._adapt_finding_to_topic(finding_text, topic_context)
            summary_bullets.append(f"• **Primary {risks_term.title()}**: {finding_text}")
        else:
            summary_bullets.append(f"• **{risks_term.title()} Assessment**: Analysis identifies potential {risks_term} requiring further investigation in {subject_area}")

        # Bullet 4: Evidence conflicts
        if conflicts:
            summary_bullets.append(f"• **Evidence Conflicts**: {len(conflicts)} significant conflicts identified between different research approaches and findings")
        else:
            summary_bullets.append("• **Evidence Consensus**: Findings show general alignment across different research sources")

        # Bullet 5: Recommendations direction - topic and audience specific
        if target_audience == "executive":
            summary_bullets.append(f"• **Strategic Direction**: Evidence supports systematic approach to {main_topic} implementation with continued monitoring")
        elif target_audience == "researcher":
            summary_bullets.append(f"• **Research Priorities**: Multiple knowledge gaps identified requiring systematic investigation in {subject_area}")
        else:  # consumer
            if topic_context['domain'] == 'health':
                summary_bullets.append(f"• **{guidance_term.title()}**: Evidence supports following established health organization recommendations")
            else:
                summary_bullets.append(f"• **{guidance_term.title()}**: Evidence supports informed decision-making based on current research findings")

        return "\n".join(summary_bullets)

    def _adapt_finding_to_topic(self, finding_text: str, topic_context: Dict[str, str]) -> str:
        """Adapt generic finding text to be topic-appropriate."""
        # Remove food-specific terms and replace with topic-appropriate language
        adaptations = {
            'ultra-processed foods': topic_context['main_topic'],
            'dietary': topic_context['subject_area'],
            'health risks': f"{topic_context['risks_term']} for {topic_context['main_topic']}",
            'health benefits': f"{topic_context['opportunities_term']} of {topic_context['main_topic']}",
            'nutritional': topic_context['subject_area'],
            'food': topic_context['main_topic'],
            'health and nutrition': topic_context['main_topic']  # For when old health context bleeds through
        }

        adapted_text = finding_text
        for old_term, new_term in adaptations.items():
            adapted_text = adapted_text.replace(old_term, new_term)

        return adapted_text

    def _generate_key_insights(self, key_findings: List[str], clusters: List[Dict],
                             evidence_list: List[Dict], target_audience: str, user_query: str) -> str:
        """Generate key insights with inline citations and confidence levels."""
        insights = []
        topic_context = self._get_topic_context(user_query)
        main_topic = topic_context['main_topic']
        subject_area = topic_context['subject_area']

        # Process each cluster for insights
        for cluster in clusters:
            cluster_label = cluster["label"]
            items = cluster["items"]

            if not items:
                continue

            # Find corresponding key finding with fuzzy matching
            cluster_finding = None
            for finding in key_findings:
                finding_lower = finding.lower()
                cluster_lower = cluster_label.lower()

                # Direct match
                if cluster_lower in finding_lower:
                    cluster_finding = finding
                    break

                # Fuzzy matching for plural/singular forms
                if cluster_lower == "methodology" and ("methodolog" in finding_lower):
                    cluster_finding = finding
                    break
                elif cluster_lower == "health risks" and ("health" in finding_lower and "risk" in finding_lower):
                    cluster_finding = finding
                    break
                elif cluster_lower == "demographics" and ("demographic" in finding_lower):
                    cluster_finding = finding
                    break

            if cluster_finding:
                # Extract citation from finding
                citation_match = re.search(r'\[([^\]]+)\]', cluster_finding)
                citations = citation_match.group(1) if citation_match else ""

                # Clean and adapt the finding text to the topic
                insight_text = cluster_finding.split('[')[0].strip()
                insight_text = self._adapt_finding_to_topic(insight_text, topic_context)

                # Generate topic-appropriate insight labels based on cluster and audience
                if target_audience == "executive":
                    if any(term in cluster_label.lower() for term in ["risk", "health", "concern"]):
                        insight_text = f"**Risk Assessment for {main_topic}**: {insight_text}"
                    elif any(term in cluster_label.lower() for term in ["recommendation", "guidance"]):
                        insight_text = f"**Strategic Implications**: {insight_text}"
                    elif "methodology" in cluster_label.lower():
                        insight_text = f"**Evidence Quality**: {insight_text}"
                    else:
                        insight_text = f"**{cluster_label} Analysis**: {insight_text}"
                elif target_audience == "researcher":
                    if "methodology" in cluster_label.lower():
                        insight_text = f"**Methodological Considerations**: {insight_text} - implications for {subject_area} research protocols"
                    elif any(term in cluster_label.lower() for term in ["gap", "limitation"]):
                        insight_text = f"**Research Opportunities**: {insight_text} - priority areas for {subject_area}"
                    else:
                        insight_text = f"**{cluster_label} Findings**: {insight_text}"
                else:  # consumer
                    if any(term in cluster_label.lower() for term in ["risk", "health", "concern"]):
                        insight_text = f"**Important Considerations**: {insight_text}"
                    elif any(term in cluster_label.lower() for term in ["benefit", "opportunity"]):
                        insight_text = f"**Potential Benefits**: {insight_text}"
                    else:
                        insight_text = f"**Key Finding**: {insight_text}"

                # Add strength indicator based on evidence quality
                quality_scores = [evidence_list[int(i)].get("quality", 0) for i in citations.split(",") if i.strip().isdigit() and int(i) < len(evidence_list)]
                avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0

                if avg_quality >= 4.5:
                    strength = "**HIGH CONFIDENCE**"
                elif avg_quality >= 3.5:
                    strength = "**MODERATE CONFIDENCE**"
                else:
                    strength = "**UNCERTAIN** - limited evidence quality"

                insights.append(f"{insight_text} [{citations}] *{strength}*")

        return "\n\n".join(insights) if insights else "*No significant insights identified from current evidence*"

    def _generate_timeline_section(self, evidence_list: List[Dict], target_audience: str) -> str:
        """Generate timeline or research evolution section with temporal analysis."""
        # Group evidence by year
        timeline_data = {}
        for i, evidence in enumerate(evidence_list):
            date = evidence.get("date")
            if date:
                try:
                    year = date.split("-")[0]
                    if year not in timeline_data:
                        timeline_data[year] = []
                    timeline_data[year].append((i, evidence))
                except:
                    pass

        if not timeline_data:
            return "## Research Evolution\n\n*Insufficient date information to generate timeline*"

        timeline_section = "## Research Timeline & Evolution\n\n"

        for year in sorted(timeline_data.keys(), reverse=True):
            year_evidence = timeline_data[year]
            timeline_section += f"**{year}**\n"

            # Summarize key developments for this year
            high_quality_evidence = [e for _, e in year_evidence if e.get("quality", 0) >= 4]

            if high_quality_evidence:
                # Group by source type
                pubmed_count = len([e for e in high_quality_evidence if "pubmed" in e.get("publisher", "").lower()])
                who_count = len([e for e in high_quality_evidence if "who" in e.get("publisher", "").lower() or "fda" in e.get("publisher", "").lower()])

                developments = []
                if pubmed_count > 0:
                    developments.append(f"{pubmed_count} peer-reviewed studies")
                if who_count > 0:
                    developments.append(f"{who_count} health organization publications")

                timeline_section += f"- {', '.join(developments)} published\n"

                # Add key finding from this year
                if year_evidence:
                    sample_evidence = year_evidence[0][1]
                    sample_snippet = sample_evidence.get("snippets", [""])[0][:100] + "..." if sample_evidence.get("snippets") else ""
                    evidence_idx = year_evidence[0][0]
                    timeline_section += f"- Key development: {sample_snippet} [{evidence_idx}]\n"

            timeline_section += "\n"

        return timeline_section

    def _generate_recommendations(self, key_findings: List[str], conflicts: List[Dict],
                                gaps: List[str], target_audience: str, user_query: str) -> str:
        """Generate actionable, risk-aware recommendations based on audience and topic."""
        recommendations = []
        topic_context = self._get_topic_context(user_query)
        main_topic = topic_context['main_topic']
        guidance_term = topic_context['guidance_term']
        domain = topic_context['domain']

        if target_audience == "executive":
            recommendations.extend([
                f"**1. Risk Assessment**: Conduct comprehensive risk analysis of current {main_topic} practices based on emerging evidence",
                f"**2. Policy Monitoring**: Establish systematic monitoring of regulatory developments and {guidance_term}",
                "**3. Stakeholder Communication**: Develop clear communication strategy addressing evidence conflicts and uncertainties"
            ])

            if conflicts:
                recommendations.append("**4. Evidence-Based Decisions**: Given conflicting evidence, adopt precautionary principle until consensus emerges")

        elif target_audience == "researcher":
            recommendations.extend([
                "**1. Methodological Standardization**: Develop standardized protocols to reduce methodological conflicts between studies",
                f"**2. Longitudinal Studies**: Prioritize long-term studies to address current evidence limitations in {main_topic}"
            ])

            # Add specific research priorities based on gaps
            for i, gap in enumerate(gaps[:3]):
                recommendations.append(f"**{i+3}. Research Priority**: {gap}")

        else:  # consumer
            if domain == 'health':
                # Keep health-specific language for health topics
                recommendations.extend([
                    "**1. Follow Established Guidelines**: Adhere to current health organization recommendations while research evolves",
                    "**2. Balanced Approach**: Consider both benefits and risks when making dietary decisions",
                    "**3. Stay Informed**: Monitor updates from reputable health organizations as evidence develops"
                ])

                if conflicts:
                    recommendations.append("**4. Consult Professionals**: Given conflicting evidence, consult healthcare providers for personalized advice")
            else:
                # Topic-appropriate language for non-health topics
                recommendations.extend([
                    f"**1. Follow Established Guidelines**: Adhere to current {guidance_term} while research evolves",
                    f"**2. Balanced Approach**: Consider both benefits and challenges when making decisions about {main_topic}",
                    "**3. Stay Informed**: Monitor updates from reputable sources as evidence develops"
                ])

                if conflicts:
                    recommendations.append("**4. Consult Experts**: Given conflicting evidence, consult domain experts for personalized guidance")

        # Add risk-aware caveats
        recommendations.append("\n**Risk Considerations:**")
        recommendations.append("- Evidence is evolving and recommendations may change as new research emerges")
        recommendations.append("- Individual circumstances may require different approaches than general recommendations")

        if conflicts:
            recommendations.append("- Conflicting evidence indicates need for cautious interpretation of findings")

        return "\n".join(recommendations)

    def _generate_limitations(self, conflicts: List[Dict], gaps: List[str],
                            coverage_stats: Dict) -> str:
        """Generate limitations and open questions section for transparency."""
        limitations = []

        # Evidence quality limitations
        total_sources = coverage_stats.get("total_evidence", 0)
        high_quality = coverage_stats.get("high_quality_sources", 0)

        if total_sources > 0:
            quality_ratio = high_quality / total_sources
            if quality_ratio < 0.7:
                limitations.append(f"**Evidence Quality**: Only {high_quality}/{total_sources} sources meet high quality standards (≥4/5)")

        # Coverage limitations
        domain_count = coverage_stats.get("domains", 0)
        if domain_count < 5:
            limitations.append(f"**Source Diversity**: Limited to {domain_count} source domains, potentially missing perspectives")

        # Conflict-based limitations
        if conflicts:
            limitations.append("**Conflicting Evidence**: Significant disagreements between studies limit certainty of conclusions")
            for conflict in conflicts[:2]:  # Show first 2 conflicts
                limitations.append(f"- {conflict.get('claim', 'Conflict identified')}: {conflict.get('reason', 'Methodological differences')}")

        # Research gaps as limitations
        limitations.append("\n**Open Questions & Research Needs:**")
        for gap in gaps:
            limitations.append(f"- {gap}")

        # Methodological limitations
        limitations.extend([
            "\n**Methodological Limitations:**",
            "- Search limited to publicly available sources",
            "- Potential language and publication bias",
            "- Time-constrained evidence collection may miss recent developments"
        ])

        return "\n".join(limitations) if limitations else "*No significant limitations identified*"

    def _generate_evidence_references(self, evidence_list: List[Dict]) -> str:
        """Generate numbered evidence reference list with quality scores."""
        references = []

        for i, evidence in enumerate(evidence_list):
            url = evidence.get("url", "")
            title = evidence.get("title", "Untitled")
            publisher = evidence.get("publisher", "Unknown")
            date = evidence.get("date", "No date")
            quality = evidence.get("quality", 0)

            # Format reference
            ref = f"[{i}] **{title}**  \n"
            ref += f"Publisher: {publisher}  \n"
            ref += f"Date: {date}  \n"
            ref += f"Quality Score: {quality}/5  \n"
            ref += f"URL: {url}"

            references.append(ref)

        return "\n\n".join(references) if references else "*No references available*"