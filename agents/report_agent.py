"""
Phase 6: Report Agent

Generates structured decision-grade reports with citations.
Creates audience-tailored reports with executive summaries and recommendations.
"""

import re
from typing import Dict, Any, List
from datetime import datetime


class ReportAgent:
    """
    Phase 6: Report Agent - Generate structured decision-grade reports with citations.

    Creates comprehensive markdown reports from compressed analysis and evidence,
    tailored for different audiences with executive summaries, insights, and recommendations.
    """

    def __init__(self):
        """Initialize the ReportAgent."""
        pass

    def generate_report(self, compressed_json: Dict[str, Any], evidence_list: List[Dict[str, Any]],
                       user_query: str, target_audience: str = "consumer") -> str:
        """
        Generate structured Markdown report from compressed analysis and evidence.

        Creates comprehensive reports with executive summaries, key insights, timeline
        analysis, recommendations, and complete evidence references.

        Args:
            compressed_json: Output from CompressConflictAgent with findings and conflicts
            evidence_list: Full list of evidence blocks for citations
            user_query: Original user query for topic extraction
            target_audience: Target audience (consumer, executive, researcher)

        Returns:
            Structured Markdown report with citations and audience-appropriate content
        """
        # Extract information from compressed analysis
        key_findings = compressed_json.get("key_findings", [])
        conflicts = compressed_json.get("conflicts", [])
        gaps = compressed_json.get("gaps", [])
        coverage_stats = compressed_json.get("coverage_stats", {})
        clusters = compressed_json.get("clusters", [])

        # Generate report sections
        executive_summary = self._generate_executive_summary(key_findings, conflicts, target_audience)
        key_insights = self._generate_key_insights(key_findings, clusters, evidence_list, target_audience)
        timeline_section = self._generate_timeline_section(evidence_list, target_audience)
        recommendations = self._generate_recommendations(key_findings, conflicts, gaps, target_audience)
        limitations = self._generate_limitations(conflicts, gaps, coverage_stats)

        # Build complete report
        report = f"""# Research Report: {self._extract_topic_from_query(user_query)}

**Target Audience:** {target_audience.title()}
**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M")}
**Evidence Sources:** {coverage_stats.get('unique_sources', 0)} unique sources across {coverage_stats.get('domains', 0)} domains

---

## Executive Summary

{executive_summary}

---

## Key Insights

{key_insights}

---

{timeline_section}

---

## Recommendations

{recommendations}

---

## Limitations & Open Questions

{limitations}

---

## Evidence References

{self._generate_evidence_references(evidence_list)}

---

*Report generated by Deep Research Agent*
"""

        return report

    def _extract_topic_from_query(self, user_query: str) -> str:
        """Extract main topic from user query for report title."""
        query_lower = user_query.lower()

        if "ultra-processed foods" in query_lower or "upf" in query_lower:
            return "Ultra-Processed Foods Evidence Analysis"
        elif "ai safety" in query_lower:
            return "AI Safety in Autonomous Vehicles"
        elif "climate change" in query_lower:
            return "Climate Change Research Analysis"
        else:
            # Generic title
            return "Research Evidence Analysis"

    def _generate_executive_summary(self, key_findings: List[str], conflicts: List[Dict],
                                  target_audience: str) -> str:
        """Generate 5-bullet executive summary tailored to audience."""
        summary_bullets = []

        # Bullet 1: Main research scope and coverage
        summary_bullets.append("• **Research Scope**: Comprehensive analysis of recent evidence with systematic evaluation of quality and relevance")

        # Bullet 2: Key positive findings (if any)
        positive_findings = [f for f in key_findings if any(word in f.lower() for word in ["benefit", "positive", "advantage"])]
        if positive_findings:
            summary_bullets.append(f"• **Potential Benefits**: {positive_findings[0].split('[')[0].strip()}")
        else:
            summary_bullets.append("• **Limited Positive Evidence**: Few documented benefits identified in current research")

        # Bullet 3: Key concerns or risks
        risk_findings = [f for f in key_findings if any(word in f.lower() for word in ["risk", "harmful", "disease", "negative"])]
        if risk_findings:
            summary_bullets.append(f"• **Primary Concerns**: {risk_findings[0].split('[')[0].strip()}")
        else:
            summary_bullets.append("• **Risk Assessment**: Analysis identifies potential areas of concern requiring further investigation")

        # Bullet 4: Evidence conflicts
        if conflicts:
            summary_bullets.append(f"• **Evidence Conflicts**: {len(conflicts)} significant conflicts identified between different research approaches and findings")
        else:
            summary_bullets.append("• **Evidence Consensus**: Findings show general alignment across different research sources")

        # Bullet 5: Recommendations direction
        if target_audience == "executive":
            summary_bullets.append("• **Strategic Direction**: Evidence supports cautious approach with continued monitoring and risk assessment")
        elif target_audience == "researcher":
            summary_bullets.append("• **Research Priorities**: Multiple knowledge gaps identified requiring systematic investigation")
        else:  # consumer
            summary_bullets.append("• **Consumer Guidance**: Evidence supports following established health organization recommendations")

        return "\n".join(summary_bullets)

    def _generate_key_insights(self, key_findings: List[str], clusters: List[Dict],
                             evidence_list: List[Dict], target_audience: str) -> str:
        """Generate key insights with inline citations and confidence levels."""
        insights = []

        # Process each cluster for insights
        for cluster in clusters:
            cluster_label = cluster["label"]
            items = cluster["items"]

            if not items:
                continue

            # Find corresponding key finding
            cluster_finding = None
            for finding in key_findings:
                if cluster_label.lower() in finding.lower():
                    cluster_finding = finding
                    break

            if cluster_finding:
                # Extract citation from finding
                citation_match = re.search(r'\[([^\]]+)\]', cluster_finding)
                citations = citation_match.group(1) if citation_match else ""

                # Generate insight based on target audience
                insight_text = cluster_finding.split('[')[0].strip()

                if target_audience == "executive":
                    if "health risks" in cluster_label.lower():
                        insight_text = f"**Risk Assessment**: {insight_text} This represents a significant business and regulatory risk factor"
                    elif "recommendations" in cluster_label.lower():
                        insight_text = f"**Regulatory Landscape**: {insight_text} indicating potential future policy changes"
                elif target_audience == "researcher":
                    if "methodology" in cluster_label.lower():
                        insight_text = f"**Methodological Considerations**: {insight_text} requiring standardized research protocols"
                    elif "gaps" in cluster_finding.lower():
                        insight_text = f"**Research Opportunities**: {insight_text} presenting clear research directions"

                # Add strength indicator based on evidence quality
                quality_scores = [evidence_list[int(i)].get("quality", 0) for i in citations.split(",") if i.strip().isdigit() and int(i) < len(evidence_list)]
                avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0

                if avg_quality >= 4.5:
                    strength = "**HIGH CONFIDENCE**"
                elif avg_quality >= 3.5:
                    strength = "**MODERATE CONFIDENCE**"
                else:
                    strength = "**UNCERTAIN** - limited evidence quality"

                insights.append(f"{insight_text} [{citations}] *{strength}*")

        return "\n\n".join(insights) if insights else "*No significant insights identified from current evidence*"

    def _generate_timeline_section(self, evidence_list: List[Dict], target_audience: str) -> str:
        """Generate timeline or research evolution section with temporal analysis."""
        # Group evidence by year
        timeline_data = {}
        for i, evidence in enumerate(evidence_list):
            date = evidence.get("date")
            if date:
                try:
                    year = date.split("-")[0]
                    if year not in timeline_data:
                        timeline_data[year] = []
                    timeline_data[year].append((i, evidence))
                except:
                    pass

        if not timeline_data:
            return "## Research Evolution\n\n*Insufficient date information to generate timeline*"

        timeline_section = "## Research Timeline & Evolution\n\n"

        for year in sorted(timeline_data.keys(), reverse=True):
            year_evidence = timeline_data[year]
            timeline_section += f"**{year}**\n"

            # Summarize key developments for this year
            high_quality_evidence = [e for _, e in year_evidence if e.get("quality", 0) >= 4]

            if high_quality_evidence:
                # Group by source type
                pubmed_count = len([e for e in high_quality_evidence if "pubmed" in e.get("publisher", "").lower()])
                who_count = len([e for e in high_quality_evidence if "who" in e.get("publisher", "").lower() or "fda" in e.get("publisher", "").lower()])

                developments = []
                if pubmed_count > 0:
                    developments.append(f"{pubmed_count} peer-reviewed studies")
                if who_count > 0:
                    developments.append(f"{who_count} health organization publications")

                timeline_section += f"- {', '.join(developments)} published\n"

                # Add key finding from this year
                if year_evidence:
                    sample_evidence = year_evidence[0][1]
                    sample_snippet = sample_evidence.get("snippets", [""])[0][:100] + "..." if sample_evidence.get("snippets") else ""
                    evidence_idx = year_evidence[0][0]
                    timeline_section += f"- Key development: {sample_snippet} [{evidence_idx}]\n"

            timeline_section += "\n"

        return timeline_section

    def _generate_recommendations(self, key_findings: List[str], conflicts: List[Dict],
                                gaps: List[str], target_audience: str) -> str:
        """Generate actionable, risk-aware recommendations based on audience."""
        recommendations = []

        if target_audience == "executive":
            recommendations.extend([
                "**1. Risk Assessment**: Conduct comprehensive risk analysis of current practices based on emerging evidence",
                "**2. Policy Monitoring**: Establish systematic monitoring of regulatory developments and health organization guidance",
                "**3. Stakeholder Communication**: Develop clear communication strategy addressing evidence conflicts and uncertainties"
            ])

            if conflicts:
                recommendations.append("**4. Evidence-Based Decisions**: Given conflicting evidence, adopt precautionary principle until consensus emerges")

        elif target_audience == "researcher":
            recommendations.extend([
                "**1. Methodological Standardization**: Develop standardized protocols to reduce methodological conflicts between studies",
                "**2. Longitudinal Studies**: Prioritize long-term studies to address current evidence limitations"
            ])

            # Add specific research priorities based on gaps
            for i, gap in enumerate(gaps[:3]):
                recommendations.append(f"**{i+3}. Research Priority**: {gap}")

        else:  # consumer
            recommendations.extend([
                "**1. Follow Established Guidelines**: Adhere to current health organization recommendations while research evolves",
                "**2. Balanced Approach**: Consider both benefits and risks when making dietary decisions",
                "**3. Stay Informed**: Monitor updates from reputable health organizations as evidence develops"
            ])

            if conflicts:
                recommendations.append("**4. Consult Professionals**: Given conflicting evidence, consult healthcare providers for personalized advice")

        # Add risk-aware caveats
        recommendations.append("\n**Risk Considerations:**")
        recommendations.append("- Evidence is evolving and recommendations may change as new research emerges")
        recommendations.append("- Individual circumstances may require different approaches than general recommendations")

        if conflicts:
            recommendations.append("- Conflicting evidence indicates need for cautious interpretation of findings")

        return "\n".join(recommendations)

    def _generate_limitations(self, conflicts: List[Dict], gaps: List[str],
                            coverage_stats: Dict) -> str:
        """Generate limitations and open questions section for transparency."""
        limitations = []

        # Evidence quality limitations
        total_sources = coverage_stats.get("total_evidence", 0)
        high_quality = coverage_stats.get("high_quality_sources", 0)

        if total_sources > 0:
            quality_ratio = high_quality / total_sources
            if quality_ratio < 0.7:
                limitations.append(f"**Evidence Quality**: Only {high_quality}/{total_sources} sources meet high quality standards (≥4/5)")

        # Coverage limitations
        domain_count = coverage_stats.get("domains", 0)
        if domain_count < 5:
            limitations.append(f"**Source Diversity**: Limited to {domain_count} source domains, potentially missing perspectives")

        # Conflict-based limitations
        if conflicts:
            limitations.append("**Conflicting Evidence**: Significant disagreements between studies limit certainty of conclusions")
            for conflict in conflicts[:2]:  # Show first 2 conflicts
                limitations.append(f"- {conflict.get('claim', 'Conflict identified')}: {conflict.get('reason', 'Methodological differences')}")

        # Research gaps as limitations
        limitations.append("\n**Open Questions & Research Needs:**")
        for gap in gaps:
            limitations.append(f"- {gap}")

        # Methodological limitations
        limitations.extend([
            "\n**Methodological Limitations:**",
            "- Search limited to publicly available sources",
            "- Potential language and publication bias",
            "- Time-constrained evidence collection may miss recent developments"
        ])

        return "\n".join(limitations) if limitations else "*No significant limitations identified*"

    def _generate_evidence_references(self, evidence_list: List[Dict]) -> str:
        """Generate numbered evidence reference list with quality scores."""
        references = []

        for i, evidence in enumerate(evidence_list):
            url = evidence.get("url", "")
            title = evidence.get("title", "Untitled")
            publisher = evidence.get("publisher", "Unknown")
            date = evidence.get("date", "No date")
            quality = evidence.get("quality", 0)

            # Format reference
            ref = f"[{i}] **{title}**  \n"
            ref += f"Publisher: {publisher}  \n"
            ref += f"Date: {date}  \n"
            ref += f"Quality Score: {quality}/5  \n"
            ref += f"URL: {url}"

            references.append(ref)

        return "\n\n".join(references) if references else "*No references available*"